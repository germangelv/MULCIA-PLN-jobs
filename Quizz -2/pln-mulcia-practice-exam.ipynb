{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924d7296",
   "metadata": {},
   "source": [
    "## Apartado 1\n",
    "Se debe implementar un sistema que permita recibir una expresión como entrada (será una expresión\n",
    "formada solo por minúsculas y sin los signos de puntuación mencionados (los seis signos con los que\n",
    "vamos a trabajar en este ejercicio y que serán siempre, el punto, la coma, los dos puntos, el punto y\n",
    "coma, el signo de cierre de interrogación y de exclamación: `.,;:?!`), y la salida será la misma\n",
    "expresión pero con los cambios correspondientes a la introducción de mayúsculas y signos de\n",
    "puntuación indicados.\n",
    "A un nivel alto de especificación podremos considerar que este método tiene esta signatura:\n",
    "`string addPunctuationBasic(string)`\n",
    "\n",
    "Es decir recibirá como entrada un string y devolverá como salida un string.\n",
    "Como primera versión de esta función addPunctuationBasic se implementará un modelo que\n",
    "simplemente cambia la primera letra por mayúscula y añade al final del string de entrada un punto.\n",
    "Por ejemplo, al ejecutar\n",
    "\n",
    "`addPunctuationBasic(“it can be a very complicated thing the ocean”)`\n",
    "se obtendrá\n",
    "\n",
    ">It can be a very complicated thing the ocean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07821f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !conda install --yes --prefix {sys.prefix} numpy\n",
    "libraryList = !{sys.executable} -m pip list\n",
    "if len(list(filter(lambda x: 'numpy ' in x, libraryList))) == 0:\n",
    "    !{sys.executable} -m pip install numpy\n",
    "if len(list(filter(lambda x: 'tqdm ' in x, libraryList))) == 0:\n",
    "    !{sys.executable} -m pip install tqdm\n",
    "if len(list(filter(lambda x: 'sklearn ' in x, libraryList))) == 0:\n",
    "    !{sys.executable} -m pip install sklearn\n",
    "    \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "438e31ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos corpus de test\n",
    "with open('PunctuationTask.test.en') as f:\n",
    "    PunctuationTaskTestEn = f.readlines()\n",
    "# Cargamos corpus de check\n",
    "with open('PunctuationTask.check.en') as f:\n",
    "    PunctuationTaskCheckEn = f.readlines()\n",
    "# Cargamos corpus de training data\n",
    "with open('PunctuationTask.train.en', encoding='utf-8') as f:\n",
    "    PunctuationTaskTrainEn = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da216c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it can be a very complicated thing the ocean \n",
      "\n",
      "It can be a very complicated thing, the ocean. \n",
      "\n",
      "And it can be a very complicated thing, what human health is. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificamos\n",
    "print(PunctuationTaskTestEn[0])\n",
    "print(PunctuationTaskCheckEn[0])\n",
    "print(PunctuationTaskTrainEn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4671b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addPunctuationBasic(line):\n",
    "    # Convertimos el primer caracter de la oración a mayuscula\n",
    "    first_letter = line[0].upper()\n",
    "    # Concatenamos el caracter convertido con la oración comenzando desde el segundo caracter a n caracter \n",
    "    formatted_line = first_letter + line[1:]\n",
    "    band = True\n",
    "    # Tratamiento de puntuacion final operando con posibles espacios a eliminar y el retorno de carro\n",
    "    if formatted_line[-1] == '\\n':\n",
    "        formatted_line = formatted_line[:-1]\n",
    "        while band == True:\n",
    "            band = False\n",
    "            if formatted_line[-1] == ' ':\n",
    "                formatted_line = formatted_line[:-1]\n",
    "                band = True\n",
    "            else:\n",
    "                band = False\n",
    "    formatted_line = formatted_line + '.'\n",
    "    return formatted_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc3f7d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vamos a probar la puntuación básica.\n",
      "We made the ocean unhappy we made people very unhappy and we made them unhealthy.\n"
     ]
    }
   ],
   "source": [
    "# Probamos con un ejemplo y con el corpus de test\n",
    "frase = 'vamos a probar la puntuación básica'\n",
    "print(addPunctuationBasic(frase))\n",
    "print(addPunctuationBasic(PunctuationTaskTestEn[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33872e54",
   "metadata": {},
   "source": [
    "## Apartado 2\n",
    "Implementar la función verifyPunctuation con la siguiente signatura\n",
    "`[(pos,err)] verifyPunctuation(string check, string test)`\n",
    "\n",
    "Para realizar esta operación se llevará a cabo una tokenización de ambos strings. En este caso se\n",
    "considerarán tokens todas las secuencias de letras, números y cualquier otro signo que no sea uno de\n",
    "los seis indicados como signos de puntuación en este ejercicio (`.,;:?!`)\n",
    "Es decir, esta función devolverá una lista de pares, donde cada par contendrá la posición (indicada\n",
    "como índice en el string check) y el tipo de error. Los errores posibles son:\n",
    "* ‘I’ → Insertion\n",
    "* ‘D’ → Deletion\n",
    "* ‘S’ → Substitution\n",
    "\n",
    "Por ejemplo, consideremos que el string de referencia correcto (check) es\n",
    "“Hello. What’s your name?”\n",
    "La tokenización generará los siguientes 6 tokens de referencia\n",
    "* Token 0: Hello\n",
    "* Token 1: .\n",
    "* Token 2: What’s\n",
    "* Token 3: your\n",
    "* Token 4: name\n",
    "* Token 5: ?\n",
    "\n",
    "Consideremos que nuestro algoritmo de puntuación (un caso hipotético para analizar el algoritmo de\n",
    "verificación) genera la siguiente salida\n",
    ">“Hello what’s your, name?”\n",
    "\n",
    "Es decir los tokens generados en este caso son:\n",
    "* Token 0: Hello\n",
    "* Token 1: what’s\n",
    "* Token 2: your\n",
    "* Token 3: ,\n",
    "* Token 4: name\n",
    "* Token 5: ?\n",
    "\n",
    "Debemos devolver la lista de cambios necesarios para convertir la cadena de tokens generados\n",
    "(hipótesis) en la cadena de tokens correctos. Para ello, podemos inspirarnos en el algoritmo de la\n",
    "distancia de Levenshtein. Es importante tener en cuenta que dadas dos cadenas A y B:\n",
    "> Dist(A,B) == Dist(B,A)\n",
    "\n",
    "Por lo que podemos abordar el problema tanto desde el punto de los cambios que hay que hacer para\n",
    "llegar desde la hipótesis hasta el modelo correcto, o bien desde el modelo correco (referencia o check\n",
    "en la terminología de este ejercicio) hasta la hipótesis generada por nuestro algoritmo de puntuación.\n",
    "Podemos ver que respecto a nuestro string de referencia (check), el string de test ha ignorado (podemos\n",
    "decir que borrado respecto al de referencia un ., por tanto tendríamos el error\n",
    ">(‘D’,1)\n",
    "\n",
    "En segundo lugar, el token 2 del string de referencia (check) se ha quedado mal en el string de test ya\n",
    "que en lugar de “What’s” aparece “what’s”. Se trata por tanto de un error de substitución de una palabra\n",
    "por otra (en este caso por un error de introducción de mayúsculas):\n",
    ">(‘S’,2)\n",
    "\n",
    "Y en tercer lugar, entre los tokens ‘your’ y ‘name’ del string correcto (check) se ha introducido un\n",
    "nuevo token en el string de test, una coma en concreto, por tanto hay un error que calificaríamos como\n",
    ">(‘I’,4)\n",
    "\n",
    "Como se puede observar, todos los errores se posicionan (usan los índices) respecto al string correcto\n",
    "de referencia (check).\n",
    "Así pues el resultado de\n",
    "\n",
    "`verifyPunctuation(“Hello. What’s your name?”,`\n",
    "`“Hello what’s your, name?”)`\n",
    "\n",
    "sería\n",
    "\n",
    "> [ (‘D’,1), (‘S’,2), (‘I’, 4) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71127441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para tokenizar\n",
    "def tokenizador(texto, tokens):\n",
    "    \n",
    "    # Verificacion strings con datos\n",
    "    if len(texto) == 0: return 0\n",
    "    texto_tokenizado = []\n",
    "    \n",
    "    # Generacion de tokens\n",
    "    start = 0\n",
    "    for i in range(len(texto)):\n",
    "        # Espacios\n",
    "        if texto[i] == ' ':\n",
    "            texto_tokenizado.append(texto[start:i])\n",
    "            start=i+1  \n",
    "        # Analisis de prueba con tokens\n",
    "        for j in range (len(tokens)):\n",
    "            # tokens\n",
    "            if texto[i] == tokens[j]:\n",
    "                texto_tokenizado.append(texto[start:i])\n",
    "                start=i\n",
    "    return texto_tokenizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "015d7972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference levenshtein https://python-course.eu/applications-python/levenshtein-distance.php\n",
    "def iterative_levenshtein(s, t):\n",
    "    \"\"\" \n",
    "        iterative_levenshtein(s, t) -> ldist\n",
    "        ldist is the Levenshtein distance between the strings \n",
    "        s and t.\n",
    "        For all i and j, dist[i,j] will contain the Levenshtein \n",
    "        distance between the first i characters of s and the \n",
    "        first j characters of t\n",
    "    \"\"\"\n",
    "\n",
    "    rows = len(s)+1\n",
    "    cols = len(t)+1\n",
    "    dist = [[0 for x in range(cols)] for x in range(rows)]\n",
    "\n",
    "    # source prefixes can be transformed into empty strings \n",
    "    # by deletions:\n",
    "    for i in range(1, rows):\n",
    "        dist[i][0] = i\n",
    "\n",
    "    # target prefixes can be created from an empty source string\n",
    "    # by inserting the characters\n",
    "    for i in range(1, cols):\n",
    "        dist[0][i] = i\n",
    "        \n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if s[row-1] == t[col-1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            dist[row][col] = min(dist[row-1][col] + 1,      # deletion\n",
    "                                 dist[row][col-1] + 1,      # insertion\n",
    "                                 dist[row-1][col-1] + cost) # substitution\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5a345d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-lectura de matriz para detectar camino optimo\n",
    "def minimal_operations(matriz_levenshtein, check_len, test_len):\n",
    "  \n",
    "    # Generando informe\n",
    "    col = test_len\n",
    "    row = check_len\n",
    "    problemas = []\n",
    "    while col > 1:\n",
    "        while row > 1:\n",
    "            # print(\"col: \"+str(col)+\" fil: \"+str(row)+\" max fil col : \"+str(check_len)+\" \"+str(test_len))\n",
    "            # print(\"D: \"+str(matriz_levenshtein[row-1][col])+\" I: \"+str(matriz_levenshtein[row][col-1])\n",
    "            #       +\" S: \"+str(matriz_levenshtein[row-1][col-1]))\n",
    "            minimo = min(matriz_levenshtein[row-1][col],    # deletion\n",
    "                         matriz_levenshtein[row][col-1],   # insertion\n",
    "                         matriz_levenshtein[row-1][col-1]) # substitution\n",
    "            # Diagonalizo\n",
    "            \n",
    "            if matriz_levenshtein[row-1][col-1] == minimo:\n",
    "                if matriz_levenshtein[row-1][col-1] == matriz_levenshtein[row][col] - 1:\n",
    "                    # Substitution\n",
    "                    problemas.append(\"S,\"+str(row))\n",
    "                col = col - 1\n",
    "                row = row - 1\n",
    "            elif matriz_levenshtein[row-1][col] == minimo:\n",
    "                # Deletion\n",
    "                problemas.append(\"D,\"+str(row))\n",
    "                row = row - 1\n",
    "            else:\n",
    "                # Insertion\n",
    "                problemas.append(\"I,\"+str(row))\n",
    "                col = col - 1\n",
    "\n",
    "    return np.flip(problemas)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "159784eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifyPunctuation(check, test, tokenizar=True, extended_report=False):\n",
    "    \"\"\" \n",
    "        Analizamos distancias según casos posibles combinando situaciones\n",
    "        Agregue reporte para poder visualizar la matriz en las pruebas\n",
    "        Agregue tokenizador para simplificar el procesamiento ya que el enunciado no es claro\n",
    "    \"\"\" \n",
    "    if(check == '' or test == ''):\n",
    "        return [(0, 'D'), (0, 'S'), (0, 'I')]\n",
    "    # Tokens reservados\n",
    "    reserve_tokens = ['.',',',';',':','?','!']\n",
    "\n",
    "    if tokenizar:\n",
    "        # Tokenizamos input\n",
    "        data_check = tokenizador(check, reserve_tokens)\n",
    "        data_test = tokenizador(test, reserve_tokens)\n",
    "    else:\n",
    "        data_check = check\n",
    "        data_test = test\n",
    "\n",
    "    matriz_levenshtein = iterative_levenshtein(data_check, data_test)\n",
    "\n",
    "    # Generar reporte\n",
    "    if extended_report:\n",
    "        # Impresion de verificacion, opcional\n",
    "        print('DATOS CORRECTOS')\n",
    "        print(data_check)\n",
    "        print('DATOS A SER PROBADOS')\n",
    "        print(data_test)\n",
    "        \n",
    "        # Impresion Matriz Levenshtein opcional\n",
    "        rows = len(data_check)+1\n",
    "        for r in range(rows):\n",
    "            print(matriz_levenshtein[r])\n",
    "            \n",
    "    # Exportar camino\n",
    "    return minimal_operations(matriz_levenshtein, len(data_check), len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee148b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATOS CORRECTOS\n",
      "Hello. What’s your name?\n",
      "DATOS A SER PROBADOS\n",
      "Hello what’s your, name?\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "[1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "[2, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "[3, 2, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "[4, 3, 2, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "[5, 4, 3, 2, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[6, 5, 4, 3, 2, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[7, 6, 5, 4, 3, 2, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "[8, 7, 6, 5, 4, 3, 2, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "[9, 8, 7, 6, 5, 4, 3, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "[10, 9, 8, 7, 6, 5, 4, 4, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "[11, 10, 9, 8, 7, 6, 5, 5, 4, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "[12, 11, 10, 9, 8, 7, 6, 6, 5, 4, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "[13, 12, 11, 10, 9, 8, 7, 7, 6, 5, 4, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[14, 13, 12, 11, 10, 9, 8, 8, 7, 6, 5, 4, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "[15, 14, 13, 12, 11, 10, 9, 9, 8, 7, 6, 5, 4, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "[16, 15, 14, 13, 12, 11, 10, 10, 9, 8, 7, 6, 5, 4, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "[17, 16, 15, 14, 13, 12, 11, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[18, 17, 16, 15, 14, 13, 12, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[19, 18, 17, 16, 15, 14, 13, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 3, 3, 4, 5, 6, 7, 8]\n",
      "[20, 19, 18, 17, 16, 15, 14, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 4, 4, 3, 4, 5, 6, 7]\n",
      "[21, 20, 19, 18, 17, 16, 15, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 5, 5, 4, 3, 4, 5, 6]\n",
      "[22, 21, 20, 19, 18, 17, 16, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 6, 6, 5, 4, 3, 4, 5]\n",
      "[23, 22, 21, 20, 19, 18, 17, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 7, 7, 6, 5, 4, 3, 4]\n",
      "[24, 23, 22, 21, 20, 19, 18, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 8, 8, 7, 6, 5, 4, 3]\n",
      "['D,6' 'S,8' 'I,18']\n",
      "DATOS CORRECTOS\n",
      "['Hello', '.', 'What’s', 'your', 'name']\n",
      "DATOS A SER PROBADOS\n",
      "['Hello', 'what’s', 'your', ',', 'name']\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "[1, 0, 1, 2, 3, 4]\n",
      "[2, 1, 1, 2, 3, 4]\n",
      "[3, 2, 2, 2, 3, 4]\n",
      "[4, 3, 3, 2, 3, 4]\n",
      "[5, 4, 4, 3, 3, 3]\n",
      "['S,2' 'S,3' 'S,4']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Prueba de Apartado 2 tokenizada y sin tokenizar\n",
    "str1 = 'Hello. What’s your name?'\n",
    "str2 = 'Hello what’s your, name?'\n",
    "\n",
    "print(verifyPunctuation(str1,str2, tokenizar = False, extended_report = True ))\n",
    "print(verifyPunctuation(str1,str2, tokenizar = True, extended_report = True))\n",
    "print(verifyPunctuation(\"Manhattan\",\"Manahaton\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a09123a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D,6' 'D,12' 'D,18']\n"
     ]
    }
   ],
   "source": [
    "# Prueba de corpus tokenizado y sin tokenizar\n",
    "print(verifyPunctuation(PunctuationTaskCheckEn[3],PunctuationTaskTestEn[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3cf9a5",
   "metadata": {},
   "source": [
    "## Apartado 3\n",
    "\n",
    "Implementar una herramienta que permita recorrer todo el corpus de test y verificación. Es decir, iría\n",
    "recorriendo una a una las líneas de cada fichero (que están alineadas), aplicaría sobre la frase de test el\n",
    "algoritmo básico de puntuación (apartado 1: `addPunctuationBasic()` ) y a continuación\n",
    "comprobaría si el resultado es o no correcto usando la función `verifyPunctuation()` del\n",
    "apartado 2.\n",
    "\n",
    "Obtener a continuación los valores relativos a `precisión`, exhaustividad (`recall`) y `F1` para el algoritmo\n",
    "`addPunctuationBasic()` implementado en el apartado 1.\n",
    "\n",
    "Consideraremos estos valores como el baseline, el modelo más básico de puntuación que podemos\n",
    "realizar para estudiar posibles mejoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "218cf716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cantidad de vees que se acierta dada toda la entrada\n",
    "def accuracy(tn, fp, fn, tp):\n",
    "    return (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "# Cuanto de lo positivo es positivo\n",
    "def precision(fp, tp):\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "# Casos veerdaderos positivos sobre todo lo positivo\n",
    "def recall_test(fn, tp):\n",
    "    return tp * 100 / (tp + fn) \n",
    "\n",
    "# Correlacion entre precision y recall\n",
    "def f1_score(recall, precision):\n",
    "    return 200 * (( recall * precision) / (recall + precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3ecd97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calculateMetrics(check, test, punctuationBasic = False):\n",
    "    \n",
    "    testProceced = []\n",
    "    # Obtengo frases de test procesadas con su puntuacion\n",
    "    # for i in tqdm(range(len(test)),ncols = 100, desc=\"Agrego puntuacion ...  \"):\n",
    "    #     testProceced.append(addPunctuationBasic(test[i]))\n",
    "    \n",
    "    test_analysis = []\n",
    "    # Obtengo fidelidad del corpus de test\n",
    "    \n",
    "    if punctuationBasic:\n",
    "        for i in tqdm(range(len(test)),ncols = 100 , desc =\"Verifico puntuacion ...\", disable = False):\n",
    "            test_analysis.append(verifyPunctuation(check[i], addPunctuationBasic(test[i])))\n",
    "    else:\n",
    "        for i in tqdm(range(len(test)),ncols = 100 , desc =\"Verifico puntuacion ...\", disable = False):\n",
    "            test_analysis.append(verifyPunctuation(check[i], test[i]))\n",
    "    \n",
    "    total = len(test)\n",
    "    correct = 0\n",
    "    \n",
    "    \n",
    "    # Como los valores verdaderos son de check, asumo todos verdaderos ya que no tengo un algoritmo de prediccion\n",
    "    y_true = np.ones(len(test))\n",
    "    \n",
    "    # Inicio mi vector de prediccion al cual voy a agregarle el resultado de mi verifyOuntuaction\n",
    "    y_pred = np.zeros(len(test))\n",
    "    \n",
    "    for i in tqdm(range(len(test)),ncols = 100 , desc =\"Observo ...\", disable = False):\n",
    "        # Si el analysis previo no genero cambios es porque mi AddPuntuactionBasic logro corregir test\n",
    "        if len(test_analysis[i]) == 0: \n",
    "            y_pred[i] = 1\n",
    "            correct = correct + 1\n",
    "        \n",
    "    # analyticslane.com/2019/10/09/numpy-basico-inicializacion-de-arrays-en-numpy/\n",
    "    # Fabrico mi matriz de confusion para las estadisticas\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    precision_data = precision(fp, tp)\n",
    "    recall = recall_test(fn, tp)\n",
    "    return precision_data, recall, f1_score(recall, precision_data) \n",
    "    \n",
    "    # print(\"Accuracy: \"+str(accuracy_data * 100))\n",
    "    # print(\"Precision: \"+str(precision_data * 100))\n",
    "    # print(\"Recall: \"+str(recall_data * 100))\n",
    "    # print(\"F1 Score: \"+str(f1_score(recall, precision_data) * 100))\n",
    "    # print(correct / total * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5373d69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifico puntuacion ...: 100%|███████████████████████████████| 14382/14382 [00:14<00:00, 991.99it/s]\n",
      "Observo ...: 100%|███████████████████████████████████████| 14382/14382 [00:00<00:00, 1308542.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Implementation metrics \n",
      " Precision: 1.0 \n",
      " Recall: 0.2920317062995411 \n",
      " F1: 45.205037132709066 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "precision, recall, f1 = calculateMetrics(PunctuationTaskCheckEn, PunctuationTaskTestEn)\n",
    "print(\"Basic Implementation metrics \\n Precision: %s \\n Recall: %s \\n F1: %s \\n\" % (precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43750914",
   "metadata": {},
   "source": [
    "## Apartado 4\n",
    "\n",
    "Utilizando el corpus de entrenamiento contenido en PunctuationTask.train.en construir un modelo de\n",
    "lenguaje inspirado en la idea de 4-gramas. No es exactamente un 4-grama pero está basado en dicho\n",
    "modelo.\n",
    "El objetivo de este pseudo 4-grama será predecir si en la posición P de un string debemos introducir un\n",
    "signo de puntuación (siempre estamos restringiendo el alcance a los seis signos de puntuación\n",
    "considerados en este ejercicio), o si debemos cambiar la palabra en dicha posición P por mayúscula.\n",
    "En última instancia, el 4-grama contendrá tuplas de la siguiente forma:\n",
    "\n",
    "\n",
    "`(token1, token2, token3, operación)`\n",
    "\n",
    "donde token1, token2 y token3 serán tokens cualesquiera incluidos en el corpus de entrenamiento. Por\n",
    "tanto estos tokens podrán ser palabras o signos de puntuación, y ‘operación’ será una de las siguientes\n",
    "operaciones:\n",
    "\n",
    "`signo de puntuación (que podrá ser uno de los seis considerados: .,;;?!`\n",
    "\n",
    "`mayúscula (que indica que la siguiente palabra se debe poner en mayúscula)`\n",
    "\n",
    "`minúscula (que indica que la siguiente palabra deberá estar en minúscula)`\n",
    "\n",
    "La operación se decide observando el fenómeno más común (frecuencia relativa) de las distintas\n",
    "operaciones para cada tríada de tokens (token1 token2 token3).\n",
    "Por ejemplo, podríamos detectar que para la tríada de tokens\n",
    "`(‘by’, ‘the’, ‘way’)`\n",
    "la operación más frecuente es insertar el signo de puntuación coma (,)\n",
    "Una vez creado este modelo de lenguaje se debe implementar una segunda versión de la función que\n",
    "añade signos de puntuación denominada\n",
    "`string addPunctuation4gram(string)`\n",
    "\n",
    "que recibirá como en el apartado 1 un string de entrada, y devolverá el nuevo string con los cambios\n",
    "introducidos aplicando el modelo de lenguaje previamente entrenado con el 4-grama previamente\n",
    "indicado.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "829d611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "token_regular_expression = r'[^.,:;?\\s]+|[.,:;?]'\n",
    "character_regular_expression = '([.,;:?!])'\n",
    "\n",
    "\n",
    "def processTrainData(train_data):\n",
    "    model_data = ' '.join(train_data).rstrip()\n",
    "    tokens = re.findall(token_regular_expression, model_data)\n",
    "    n_grams = []\n",
    "    for i in range(0, len(tokens)-4):\n",
    "        j = i+4\n",
    "        n_grams.append(tokens[i:j])\n",
    "    train_grams = np.array(n_grams)\n",
    "\n",
    "    output = {}\n",
    "    for n_gram in train_grams:\n",
    "        word_ngram = n_gram[3]\n",
    "        word_key = ''.join(n_gram[0:3])\n",
    "        next = ''\n",
    "        if re.search(character_regular_expression, word_ngram):\n",
    "            next = 'C ' + word_ngram\n",
    "        elif word_ngram[0].isupper():\n",
    "            next = 'M'\n",
    "        elif word_ngram[0].isupper() != True:\n",
    "            next = 'm'\n",
    "\n",
    "        if word_key in output:\n",
    "            output[word_key] = np.append(output[word_key], next)\n",
    "        else:\n",
    "            output[word_key] = np.array([next])\n",
    "\n",
    "    for word_key in output:\n",
    "        unique, counts = np.unique(output[word_key], return_counts=True)\n",
    "        common = np.where(counts == max(counts))\n",
    "        output[word_key] = unique[common]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2def03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Entrenando ...: 100%|█████████████████████████████████████| 14382/14382 [00:00<00:00, 19632.83it/s]\n"
     ]
    }
   ],
   "source": [
    "def addPunctuation4gram(test_data, train_data):\n",
    "    \n",
    "    train_data = processTrainData(train_data)\n",
    "\n",
    "    output = []\n",
    "    for w in tqdm(range(len(test_data)),ncols = 100 , desc =\" Entrenando ...\", disable = False):\n",
    "        tokens = re.findall(token_regular_expression, test_data[w])\n",
    "        i = 0\n",
    "        while i < len(tokens)-4:\n",
    "            sentence = tokens[i: i+4]\n",
    "            items = sentence[0:3]\n",
    "            key =  ''.join(items)\n",
    "            if key in train_data:\n",
    "                value = train_data[key][0]\n",
    "\n",
    "                if 'C' in value:\n",
    "                    target_token = tokens[i+3]\n",
    "                    character =  value.split()[1]\n",
    "                    if target_token != character:\n",
    "                        tokens.insert(i+3,character)\n",
    "                elif 'M' in value:\n",
    "                    target_token = tokens[i+3]\n",
    "                    if target_token[0].isupper() != True:\n",
    "                        tokens[i+3] = target_token.capitalize()\n",
    "                elif 'm' in value:\n",
    "                    target_token = tokens[i+3]\n",
    "                    if target_token[0].isupper():\n",
    "                        tokens[i+3] = target_token.lower()\n",
    "\n",
    "            i+=1\n",
    "            \n",
    "        transformation = ' '.join(tokens)\n",
    "        transformation = addPunctuationBasic(transformation)\n",
    "        output.append(transformation)\n",
    "    \n",
    "    return output\n",
    "\n",
    "punctuation4gram_data = addPunctuation4gram(PunctuationTaskTestEn, PunctuationTaskTrainEn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d5a05f",
   "metadata": {},
   "source": [
    "## Apartado 5\n",
    "\n",
    "Aplicar el modelo de verificación implementado en el apartado 2, pero contrastando el corpus de\n",
    "referencia con el resultado generado por el algoritmo de puntuación basado en 4-gramas del apartado 4.\n",
    "A continuación obtener los valores de precisión, exhaustividad (recall) y F1 sobre estos nuevos\n",
    "resultados y compararlos con los obtenidos en el apartado 3.\n",
    "Este nuevo algoritmo de puntuación basado en 4-gramas, ¿mejora los resultados? Analiza si mejora o\n",
    "empeora, ¿por qué puede ocurrir?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7cf1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifico puntuacion ...:  52%|████████████████▋               | 7524/14382 [00:07<00:13, 521.96it/s]"
     ]
    }
   ],
   "source": [
    "#Original\n",
    "# precision, recall, f1 = calculateMetrics(PunctuationTaskCheckEn, PunctuationTaskTestEn)\n",
    "# print(\"Basic Implementation metrics \\n Precision: %s \\n Recall: %s \\n F1: %s \\n\" % (precision, recall, f1))\n",
    "\n",
    "#4-grams\n",
    "n_precision, n_recall, n_f1 = calculateMetrics(PunctuationTaskCheckEn, punctuation4gram_data, punctuationBasic = False)\n",
    "print(\"4-grams metrics \\n Precision: %s \\n Recall: %s \\n F1: %s \\n\" % (n_precision, n_recall, n_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78853924",
   "metadata": {},
   "source": [
    "## Apartado 6\n",
    "\n",
    "Utilizando también ejemplos de TED talks, en un artículo de 2016, Ottokar Tilk y Tanel Alum ha\n",
    "aplicado un modelo de redes recurrentes bidireccionales para esta misma tarea (restauración de signos\n",
    "de puntuación en textos no segmentados).\n",
    "El artículo donde lo describen se encuentra publicado en este enlace:\n",
    "\n",
    "* https://www.isca-speech.org/archive/Interspeech_2016/pdfs/1517.PDF\n",
    "* Bidirectional Recurrent Neural Network with Attention Mechanism forPunctuation Restoration (InterSpeech 2016).\n",
    "\n",
    "El código correspondiente a su implementación se encuentra también disponible en github:\n",
    "\n",
    "* https://github.com/ottokart/punctuator2\n",
    "\n",
    "Los resultados de la evaluación para tres signos de puntuación concretos han sido estos (disponibles en\n",
    "la dirección github anterior):\n",
    "\n",
    "PUNCTUATION PRECISION RECALL F-SCORE\n",
    ",COMMA 64.4 45.2 53.1\n",
    "?QUESTIONMARK 67.5 58.7 62.8\n",
    ".PERIOD 72.3 71.5 71.9\n",
    "Overall 68.9 58.1 63.1\n",
    "\n",
    "El objetivo de este apartado es estudiar la implementación que han hecho estos autores con una red\n",
    "neuronal y adecuarla al problema que se ha planteado en este ejercicio.\n",
    "\n",
    "En concreto, este apartado consistirá en la adecuación del modelo de Tilk y Alum al escenario de este\n",
    "ejercicio. Es decir, aplicar el entrenamiento de la red propuesta por estos autores al corpus de\n",
    "entrenamiento (PuntuationTask.train.en) y a continuación evaluar el resultado obtenido usando el\n",
    "modelo de verificación implementado previamente.\n",
    "\n",
    "Los resultados que obtienes para tu evaluación son similares a los publicados por estos autores.\n",
    "\n",
    "Nota: Observar el modelo implementado por estos autores en el script error_calculator.py, como\n",
    "contraste a vuestro algoritmo de verificación (apartado 2) y evaluación (apartados 3 y 5).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd6a5c",
   "metadata": {},
   "source": [
    "## Apartado 7\n",
    "\n",
    "A partir del trabajo realizado y teniendo en cuenta otros enfoques disponibles en el ámbito de las\n",
    "tecnologías del lenguaje, investiga qué otros enfoques o estrategias alternativas para esta misma tarea.\n",
    "El objetivo de este apartado es que busques bibliografía relevante reciente sobre esta tarea, selecciones\n",
    "uno o dos artículos y describas brevemente el enfoque y resultados obtenidos.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iacd",
   "language": "python",
   "name": "iacd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
